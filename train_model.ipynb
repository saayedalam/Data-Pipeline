{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_model.py (5 points) \n",
    "When this is called using python train_model.py in the command line, this will take in the training dataset csv, perform the necessary data cleaning and imputation, and fit a classification model to the dependent Y. There must be data check steps and clear commenting for each step inside the .py file. The output for running this file is the random forest model saved as a .pkl file in the local directory. Remember that the thought process and decision for why you chose the final model must be clearly documented in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following libraries will be used to answer this question.\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# There are some warnings about future changes to the packages. Those warnings will be suppressed.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to fit the classifier\n",
    "def fit_model(X, y):\n",
    "    try:\n",
    "        # Missing values are imputed using most frequent.\n",
    "        # Embarked is the only variable with missing values after data cleaning function is run.\n",
    "        # S factor has the most occurrence. \n",
    "        imputer = Imputer(missing_values = 'NaN', strategy = 'most_frequent', axis = 0)\n",
    "        \n",
    "        # As per recommendation, random forest classifier is initialized\n",
    "        rfc = RandomForestClassifier()\n",
    "        \n",
    "        # Next, steps and pipeline are initialized\n",
    "        steps = [('imputation', imputer), ('random_forest', rfc)]\n",
    "        pipeline = Pipeline(steps)\n",
    "        \n",
    "        # Data is split in 70:30 ratio and stored in X and y variables. \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 104)\n",
    "        X_test.to_csv('X_test.csv')\n",
    "        y_test.to_csv('y_test.csv')\n",
    "        \n",
    "        # Finally, fit the data.\n",
    "        model = pipeline.fit(X_train, y_train)\n",
    "        \n",
    "    except:\n",
    "        # Display message if the there's a failure.\n",
    "        print(\"Check pipeline setup, fit or csv file save.\")\n",
    "        \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function cleans the dataset.\n",
    "def clean_data(df):\n",
    "    dummy_vars = ['Sex', 'Embarked', 'Pclass']\n",
    "    drop_vars = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Parch', 'family_size', 'female', 'C', 1]\n",
    "    \n",
    "    # Sex, Passenger Class, Embarked are categorical variables. \n",
    "    # Therefore, they are dummified and one of the variable from each are dropped.\n",
    "    for col in dummy_vars:\n",
    "        new_vars = pd.get_dummies(df[col])\n",
    "        df = df.join(new_vars)\n",
    "    \n",
    "    # A new variable is created from siblings and parents variables \n",
    "    for col in df:\n",
    "        df['family_size'] = df['SibSp'] + df['Parch'] + 1\n",
    "        \n",
    "    for col in df:\n",
    "        df['is_alone'] = 0\n",
    "        df.loc[df['family_size'] == 1, 'is_alone'] = 1\n",
    "    \n",
    "    # Age variables has several missing values. It will be filled with the mean of the age variable.\n",
    "    for col in df:\n",
    "        df['Age'].fillna(df['Age'].mean(), inplace = True)    \n",
    "        \n",
    "    # Several variables are dropped as they are not needed or has high number of missing values.    \n",
    "    df = df.drop(dummy_vars, axis = 1)\n",
    "    return df.drop(drop_vars, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function reads the data and runs the two functions above and saves the model object.\n",
    "def train_model(x):\n",
    "    # Reads and runs the clean_data function\n",
    "    df = pd.read_csv(x + \".csv\")\n",
    "    clean_df = clean_data(df)\n",
    "    \n",
    "    # Remove the dependent variable to fit the model\n",
    "    y = clean_df['Survived']\n",
    "    X = clean_df.drop('Survived', axis=1)\n",
    "    \n",
    "    # Runs the fit_model function on the cleaned data.\n",
    "    model = fit_model(X,y)\n",
    "    \n",
    "    # Saves the model object locally using the pickle package.\n",
    "    p = open('model.pkl', 'wb')\n",
    "    pickle.dump(model, p)\n",
    "    p.close()\n",
    "    \n",
    "    # Check is the object is saved locally.\n",
    "    print(\"Model Pickle Created: \" + str(path.exists('model.pkl')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Pickle Created: True\n"
     ]
    }
   ],
   "source": [
    "# Test if the functions work\n",
    "train_model('train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
