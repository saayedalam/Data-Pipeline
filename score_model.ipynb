{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score_model.py  \n",
    "When this is called using python score_model.py in the command line, this will ingest the .pkl random forest file and apply the model to the locally saved scoring dataset csv. There must be data check steps and clear commenting for each step inside the .py file. The output for running this file is a csv file with the predicted score, as well as a png or text file output that contains the model accuracy report (e.g. sklearn's classification report or any other way of model evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following libraries will be used to answer this question.\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to open random forest object\n",
    "def read_pickle_model(file):\n",
    "    try:\n",
    "        # Reads the model pickle file \n",
    "        p = open(file, 'rb')\n",
    "        open_pkl = pickle.load(p)\n",
    "        \n",
    "    except:\n",
    "        # Display error message if there's error opening the file\n",
    "        print(\"Issue opening\", file)\n",
    "        \n",
    "    return(open_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean_data function from previous section will be used to clean the data.\n",
    "# The following function cleans the dataset.\n",
    "def clean_data(df):\n",
    "    dummy_vars = ['Sex', 'Embarked', 'Pclass']\n",
    "    drop_vars = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Parch', 'family_size', 'female', 'C', 1]\n",
    "    \n",
    "    # Sex, Passenger Class, Embarked are categorical variables. \n",
    "    # Therefore, they are dummified and one of the variable from each are dropped.\n",
    "    for col in dummy_vars:\n",
    "        new_vars = pd.get_dummies(df[col])\n",
    "        df = df.join(new_vars)\n",
    "    \n",
    "    # A new variable is created from siblings and parents variables \n",
    "    for col in df:\n",
    "        df['family_size'] = df['SibSp'] + df['Parch'] + 1\n",
    "        \n",
    "    for col in df:\n",
    "        df['is_alone'] = 0\n",
    "        df.loc[df['family_size'] == 1, 'is_alone'] = 1\n",
    "    \n",
    "    # Age variables has several missing values. It will be filled with the mean of the age variable.\n",
    "    for col in df:\n",
    "        df['Age'].fillna(df['Age'].mean(), inplace = True)    \n",
    "        \n",
    "    # Several variables are dropped as they are not needed or has high number of missing values.    \n",
    "    df = df.drop(dummy_vars, axis = 1)\n",
    "    return df.drop(drop_vars, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function applies the model to the locally saved scoring dataset.\n",
    "def model_report(test, X_test, y_test):\n",
    "    # loads the datasets\n",
    "    test = pd.read_csv(test + \".csv\")\n",
    "    X_test = pd.read_csv(X_test + '.csv')\n",
    "    y_test = pd.read_csv(y_test + '.csv')\n",
    "    \n",
    "    # Cleans the dataset to run the model\n",
    "    X_test.drop(X_test.columns[[0]], axis = 1, inplace = True)\n",
    "    y_test.drop(y_test.columns[[0]], axis = 1, inplace = True)\n",
    "    X_test.drop(X_test.index[0], inplace = True)\n",
    "    \n",
    "    # Model accuracy report\n",
    "    accuracy_score = model.score(X_test, y_test)\n",
    "    \n",
    "    # Create a csv file of the score and save locally\n",
    "    score = pd.Series({'score': accuracy_score})\n",
    "    with open('score.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(score)\n",
    "\n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_test, model.predict(X_test))\n",
    "    \n",
    "    # Create a text file of the report and save locally\n",
    "    with open('report.txt', 'w') as f:\n",
    "        f.write(class_report)\n",
    "        f.close()\n",
    "    \n",
    "    # Also prints the report\n",
    "    print(\"\\n* Model accuracy score:\")\n",
    "    print(accuracy_score)\n",
    "    \n",
    "    print(\"\\n* Classification report:\\n\")\n",
    "    print(class_report)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Model accuracy score:\n",
      "0.7677902621722846\n",
      "\n",
      "* Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82       172\n",
      "           1       0.67      0.69      0.68        95\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       267\n",
      "   macro avg       0.75      0.75      0.75       267\n",
      "weighted avg       0.77      0.77      0.77       267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test if the function works\n",
    "model = read_pickle_model('model.pkl')\n",
    "model_report('test', 'X_test', 'y_test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
